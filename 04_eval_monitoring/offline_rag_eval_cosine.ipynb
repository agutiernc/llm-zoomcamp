{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d65e16c-4c7e-46ca-b25a-32eba9aa7b4b",
   "metadata": {},
   "source": [
    "# Cosine Similarity - Vid 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f294ce58-1bfa-47b9-a999-763db01a7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc064e4-1417-460f-af95-ec53cee630bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_gpt4o = pd.read_csv('results-gpt4o.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b654142-7595-4c95-8101-af3a71d043de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt4o = df_results_gpt4o.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda0b809-0e8f-42f5-b501-f937f2366e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = results_gpt4o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090b2f51-bb14-46e7-9875-38f938b4654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca1b6d1-6970-4fb7-abf4-670674a20f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda491d1-6d27-43db-9711-1269186d16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3a8750-0c88-4ac4-9d49-e6c284c47038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69346dcf-1264-4693-b076-26fa4a4b2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"question_text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21071f6-1c25-4b63-9c8e-5f53356cde69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02eecb20-15a5-4a04-bd86-bd5da7565424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5780ffc4-ccb8-44b6-a391-d87ce94549d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c597aced53574d7bad66c0b85a008467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5eab89-dd7e-4a07-a644-7fa663b9f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1830.000000\n",
       "mean        0.679129\n",
       "std         0.217995\n",
       "min        -0.153425\n",
       "25%         0.591460\n",
       "50%         0.734788\n",
       "75%         0.835390\n",
       "max         0.995339\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_gpt4o['cosine'] = similarity\n",
    "df_results_gpt4o['cosine'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344be286-9dbd-48a9-bc16-d427c7635033",
   "metadata": {},
   "source": [
    "# W/ gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfa1a73-0eaf-431c-a908-ea82af66859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_gpt35 = pd.read_csv('results-gpt35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00602d0-7077-4cb2-b160-e58456b498a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt35 = df_results_gpt35.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c9203e-984a-4422-a404-7f5a2a037bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa5794727cb4a65b4602daa2b023069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_35 = []\n",
    "\n",
    "for record in tqdm(results_gpt35):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity_35.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da30417-faf6-468d-9d30-c52f73dc8bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1830.000000\n",
       "mean        0.657599\n",
       "std         0.226062\n",
       "min        -0.168921\n",
       "25%         0.546504\n",
       "50%         0.714784\n",
       "75%         0.817262\n",
       "max         1.000000\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_gpt35['cosine'] = similarity_35\n",
    "df_results_gpt35['cosine'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29ce90-578a-48ec-8e77-bb2e59b4ccb4",
   "metadata": {},
   "source": [
    "# LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604fda04-0ee7-4c65-95c2-42ccefca3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e67ae27-c0a9-4410-be8a-833b529cb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_gpt4o_mini = pd.read_csv('results-gpt4o-mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11bc83cb-7e2e-43e6-a6ff-0b3fea87d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_results_gpt4o_mini.sample(n=150, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6977c654-557a-463c-9690-f41b78241f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1026ffd-6c21-4957-b7c4-dfe0e79bb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0cfcc6a-7cd3-4c90-8662-d0f8425c2541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'The syntax for using `precision_recall_fscore_support` in Python is as follows:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```',\n",
       " 'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       " 'document': '403bbdd8',\n",
       " 'question': 'What is the syntax for using precision_recall_fscore_support in Python?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fd813f8-b8ba-40e5-8387-b4c2a6ede5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
      "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
      "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Original Answer: Scikit-learn offers another way: precision_recall_fscore_support\n",
      "Example:\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "(Gopakumar Gopinathan)\n",
      "Generated Question: What is the syntax for using precision_recall_fscore_support in Python?\n",
      "Generated Answer: The syntax for using `precision_recall_fscore_support` in Python is as follows:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "```\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the original\n",
      "answer and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt1_template.format(**record)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8e516-c6ae-498f-b02a-6d6a422f0ee5",
   "metadata": {},
   "source": [
    "### code below uses openAI's gpt-4o-mini model\n",
    "*Did not run code and the llm() is from a previous notebook*\n",
    "\n",
    "```python\n",
    "\n",
    "answer = llm(prompt, model='gpt-4o-mini')\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(samples):\n",
    "    prompt = prompt1_template.format(**record)\n",
    "    evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "    evaluations.append(evaluation)\n",
    "\n",
    "json_evaluations = []\n",
    "\n",
    "for i, str_eval in enumerate(evaluations):\n",
    "    json_eval = json.loads(str_eval)\n",
    "    json_evaluations.append(json_eval)\n",
    "\n",
    "df_evaluations = pd.DataFrame(json_evaluations)\n",
    "\n",
    "# I named \"df_evaluations\" to \"df_evaluations_aqa\" as it's the first prompt => answer - question - answer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d426c5c3-3abf-44e3-83ed-7df753279414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_aqa = pd.read_csv('evaluations-aqa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "772fdbba-c373-4604-88ae-954e43d4bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relevance\n",
       "RELEVANT           124\n",
       "PARTLY_RELEVANT     16\n",
       "NON_RELEVANT        10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_aqa.Relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5758aa5-6397-48f1-9942-03ccdb1e8928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer discusses a pip version e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer incorrectly states that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer provides information abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the orig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer responds to a question ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the topi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer discusses the recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer addresses a different iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not relate to the to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Relevance                                        Explanation\n",
       "4    NON_RELEVANT  The generated answer discusses a pip version e...\n",
       "11   NON_RELEVANT  The generated answer does not address the spec...\n",
       "27   NON_RELEVANT  The generated answer incorrectly states that t...\n",
       "41   NON_RELEVANT  The generated answer provides information abou...\n",
       "87   NON_RELEVANT  The generated answer does not address the orig...\n",
       "90   NON_RELEVANT  The generated answer responds to a question ab...\n",
       "93   NON_RELEVANT  The generated answer does not address the topi...\n",
       "116  NON_RELEVANT  The generated answer discusses the recommended...\n",
       "138  NON_RELEVANT  The generated answer addresses a different iss...\n",
       "139  NON_RELEVANT  The generated answer does not relate to the to..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_aqa[df_evaluations_aqa.Relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ca0ac87-bbb7-4398-a736-cb7107e82eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': \"The cause of the pip version error in this week's serverless deep learning section could be a version conflict in Scikit-Learn. Specifically, if you are using a different version than what was used during the model training, it can lead to warnings and potential breaking code or invalid results. To resolve this, make sure to use the same version of Scikit-Learn that was used for training the model. For instance, if you trained with version 1.1.1, you should use that same version in your virtual environment.\",\n",
       " 'answer_orig': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto',\n",
       " 'document': '42c09143',\n",
       " 'question': \"What might be the cause of the pip version error in this week's serverless deep learning section?\",\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc3d96-4630-4811-9bb1-5d3c56932659",
   "metadata": {},
   "source": [
    "## 2nd Prompt: Question - Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90de80d9-f003-406d-afe6-73d8fa5c482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: What is the syntax for using precision_recall_fscore_support in Python?\n",
      "Generated Answer: The syntax for using `precision_recall_fscore_support` in Python is as follows:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "```\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(**record)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3e415-28ae-4c63-8a27-a6b054444368",
   "metadata": {},
   "source": [
    "### Did not run code and the llm() is from a previous notebook\n",
    "\n",
    "```python\n",
    "\n",
    "evaluations_2 = []\n",
    "\n",
    "for record in tqdm(samples):\n",
    "    prompt = prompt2_template.format(**record)\n",
    "    evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "    evaluations_2.append(evaluation)\n",
    "\n",
    "\n",
    "json_evaluations_2 = []\n",
    "\n",
    "for i, str_eval in enumerate(evaluations_2):\n",
    "    json_eval = json.loads(str_eval)\n",
    "    json_evaluations_2.append(json_eval)\n",
    "\n",
    "df_evaluations_2 = pd.DataFrame(json_evaluations_2)\n",
    "\n",
    "```\n",
    "Named `df_evaluations_2` to `df_evaluations_2_qa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9576f2e6-57e2-4a82-97f8-a625fd3dde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_2_qa = pd.read_csv('evaluations-qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b3ed9ed-5d5b-436c-b867-0a7045510f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relevance\n",
       "RELEVANT           129\n",
       "PARTLY_RELEVANT     18\n",
       "NON_RELEVANT         3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_2_qa.Relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0c278f1-c410-497b-8aa7-bf7d631e3dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer provides information abou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Relevance                                        Explanation\n",
       "45   NON_RELEVANT  The generated answer does not address the ques...\n",
       "49   NON_RELEVANT  The generated answer explicitly states that th...\n",
       "139  NON_RELEVANT  The generated answer provides information abou..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_2_qa[df_evaluations_2_qa.Relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07498f02-2a2c-4b92-b09f-de3bda173720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'The provided context does not include specific commands to start the Docker daemon on Linux. Therefore, I cannot provide an answer based solely on the facts from the context.',\n",
       " 'answer_orig': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf youâ€™re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi',\n",
       " 'document': '4b2a3181',\n",
       " 'question': 'What commands should I use to start the docker daemon on Linux?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b48f4f-47ad-4d8d-8d46-f9844e95d9a8",
   "metadata": {},
   "source": [
    "# Saving all the dats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af6a36-e396-4a08-8ff3-6724331a99bc",
   "metadata": {},
   "source": [
    "### export the data --- which I already used for this notebook\n",
    "\n",
    "```python\n",
    "\n",
    "df_gpt4o.to_csv('data/results-gpt4o-cosine.csv', index=False)\n",
    "df_gpt35.to_csv('data/results-gpt35-cosine.csv', index=False)\n",
    "df_gpt4o_mini.to_csv('data/results-gpt4o-mini-cosine.csv', index=False)\n",
    "\n",
    "df_evaluations.to_csv('data/evaluations-aqa.csv', index=False)\n",
    "df_evaluations_2.to_csv('data/evaluations-qa.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
